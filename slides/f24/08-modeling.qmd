---
title: "Modeling I"
subtitle: "POL51"
format: 
  clean-revealjs:
    incremental: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Juan Tellez
    email: jftellez@ucdavis.edu
    affiliations: UC Davis
date: last-modified
title-slide-attributes:
  data-background-image: ../img/dubois-spiral-2.png
  data-background-size: contain
  data-background-position: right
---


```{r setup, include=FALSE}
# set knit options
knitr::opts_chunk$set(
  fig.width=9, 
  fig.height=5, 
  fig.retina=3,
  fig.align="center",
  tidy = FALSE,
  tidy.opts = list(width.cutoff = 80),
  out.width = "100%",
  cache = FALSE,
  echo = FALSE,
  message = FALSE, 
  warning = FALSE
)


# source
source(here::here("R/funcs.R"))

# libraries
library(juanr)
library(socviz)

```




# Plan for today {.center background-color="`r full`"}


Correlations

. . .


Models and lines

. . .

Fitting models

. . .

## Patterns in the data

::: columns
::: {.column width="50%"}
- So far: wrangling and visualizing data
- Looking for **patterns** (hopefully real)
- Examples?
:::

::: {.column width="50%"}
```{r}
knitr::include_graphics(here::here("slides", "img", "pattern.jpeg"))
```

:::
:::


## Patterns and correlations

```{r}
knitr::include_graphics(here::here("slides", "img", "correlation.png"))
```


A useful way to talk about these patterns is in terms of **correlations**

. . .

When we see a pattern between two (or more) variables, we say they are **correlated**; when we don't see one, we say they are **uncorrelated**

. . .

Correlations can be **strong** or **weak**, **positive** or **negative**


## Correlations: strength

When two variables "move together", the correlation is **strong**

When they don't "move together", the correlation is **weak**

```{r}
p1 = county_data %>% 
  select(travel_time, per_dem_2016) %>% 
  mutate(group = "Weak correlation") %>% 
  ggplot(aes(x = travel_time, y = per_dem_2016)) + 
  geom_point(alpha = .7, color = red) + 
  labs(x = "Mean travel time to work (minutes)", 
       y = "Democratic Presidential vote, 2016") + 
  scale_y_continuous(labels = scales::percent) + 
  facet_wrap(vars(group))

p2 = county_data %>% 
  select(per_dem_2012, per_dem_2016) %>% 
  mutate(group = "Strong correlation") %>% 
  ggplot(aes(x = per_dem_2012, y = per_dem_2016)) + 
  geom_point(alpha = .7, color = red) + 
  labs(x = "Democratic Presidential vote, 2012", 
       y = NULL) + 
  scale_y_continuous(labels = scales::percent) + 
  scale_x_continuous(labels = scales::percent) + 
  facet_wrap(vars(group))

p1 + p2
```


## Correlations: strength


Better heuristic: knowing about one variable tells you *something* about the other

. . .


::: columns
::: {.column width="50%"}
**Strong**: if you know how a county voted in 2012, you have a good guess for 2016

**Weak**: if you know a county's average commute time, you know almost nothing about how it votes
:::

::: {.column width="50%"}
```{r, out.width="90%"}
p1 + p2
```
:::
:::








## Correlation: strength

We also talk (informally) about correlations that include **categorical** variables

Big gaps between groups $\rightarrow$ correlated

. . .


```{r, out.width="70%"}
p1 = therm %>% 
  filter(party_id %in% c("Democrat", "Republican")) %>% 
  group_by(party_id) %>% 
  summarise(avg = mean(ft_muslim, na.rm = TRUE)) %>% 
  mutate(group = "Muslims") %>% 
  ggplot(aes(x = party_id, y = avg, fill = party_id)) + 
  geom_col(alpha = .8, color = "white") + 
  scale_fill_brewer(palette = "Set1", direction = -1) + 
  theme(legend.position = "none") + 
  facet_wrap(vars(group)) + 
  labs(x = NULL, y = "Average thermometer") + 
  scale_y_continuous(labels = scales::percent_format(scale = 1))

p2 = therm %>% 
  filter(party_id %in% c("Democrat", "Republican")) %>% 
  group_by(party_id) %>% 
  summarise(avg = mean(ft_jew, na.rm = TRUE)) %>% 
  mutate(group = "Jewish people") %>% 
  ggplot(aes(x = party_id, y = avg, fill = party_id)) + 
  geom_col(alpha = .8, color = "white") + 
  scale_fill_brewer(palette = "Set1", direction = -1) + 
  theme(legend.position = "none") + 
  facet_wrap(vars(group)) + 
  labs(x = NULL, y = "Average thermometer") + 
  scale_y_continuous(labels = scales::percent_format(scale = 1))

p1 + p2
```





## Correlation: direction


âž•: When two variables move "in the same direction"

âž–: When two variables move "in opposite directions"


```{r}
p1 = county_data %>%
  mutate(group = "Negative correlation") %>% 
  ggplot(aes(x = per_dem_2012, y = per_gop_2012)) + 
  geom_point(fill = red, shape = 21, color = "white") + 
  labs(x = "Democratic Presidential vote, 2012", 
       y = "Republican Presidential vote, 2012") + 
  scale_y_continuous(labels = scales::percent) + 
  scale_x_continuous(labels = scales::percent) + 
  facet_wrap(vars(group))

p2 = county_data %>% 
  mutate(group = "Positive correlation") %>% 
  ggplot(aes(x = per_dem_2012, y = per_dem_2016)) + 
  geom_point(fill = red, shape = 21, color = "white") + 
  labs(x = "Democratic Presidential vote, 2012", 
       y = "Democratic Presidential vote, 2016") + 
  scale_y_continuous(labels = scales::percent) + 
  scale_x_continuous(labels = scales::percent) + 
  facet_wrap(vars(group))

p2 + p1
```



## Correlation: direction {visibility="hidden"}

âž•: counties where Dems did well in 2012 are also the counties where they did well in 2016

âž–: counties where Dems did well in 2012 are the counties where the Reps did poorly in 2012


```{r}
p2 + p1
```




## Quantifying correlation

When the two variables are continuous, we can quantify the correlation


$$r =
  \frac{ \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y}) }{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2\sum_{i=1}^{n}(y_i-\bar{y})^2}}$$


. . .

Correlation coefficient, **r**: 

ranges from -1 to 1, 

perfectly correlated = 1 or -1, 

perfectly uncorrelated = 0



. . .

All of the movement is here: $\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})$




## Quantifying correlation


```{r}
child_table = gss_sm |> 
  mutate(age_avg = mean(age, na.rm = TRUE),
         child_avg = mean(childs, na.rm = TRUE),
         diff_age = age - age_avg,
         diff_child = childs - child_avg,
         product = diff_age * diff_child) |> 
  sample_n(5) |> 
  select(age, "\\(\\overline{age}\\)" = age_avg, 
        "\\(\\ age_{i} - \\overline{age}\\)" = diff_age,
         child = childs, "\\(\\overline{child}\\)" = child_avg, 
         "\\(\\ child_{i} - \\overline{child}\\)" = diff_child,
         product) |> 
  slice(c(1, 3, 5))


kbl(child_table, digits = 2, escape = FALSE)
```


$$\sum_{i=1}^{n}(age_i-\overline{age})(child_i-\overline{child})$$

. . .

Lots of big, positive **products** $\rightarrow$ strong **positive correlation**

. . .

Lots of big, negative **products** $\rightarrow$ strong **negative correlation**


. . . 

A mix of positive and negative **products** $\rightarrow$ **weak** correlation



## Interpreting the coefficient



::: columns
::: {.column width="50%"}
```{r}
tribble(~`Correlation coefficient`, ~`Rough meaning`, 
        "+/- 0.1 - 0.3", "Modest", 
        "+/- 0.3 - 0.5", "Moderate", 
        "+/- 0.5 - 0.8", "Strong", 
        "+/- 0.8 - 0.9", "Very strong") %>% 
  knitr::kable()
```
:::

::: {.column width="50%"}
```{r correlation-grid, echo=FALSE, fig.dim=c(4.8, 4.2), out.width="100%"}
make_correlated_data <- function(r, n = 200) {
  MASS::mvrnorm(n = n, 
                mu = c(0, 0), 
                Sigma = matrix(c(1, r, r, 1), nrow = 2), 
                empirical = TRUE) %>% 
    magrittr::set_colnames(c("x", "y")) %>% 
    as_tibble()
}
cor_grid <- tibble(r = c(0.2, 0.4, 0.75, 0.9)) %>% 
  mutate(data = map(r, make_correlated_data)) %>% 
  unnest(data)
ggplot(cor_grid, aes(x = x, y = y)) +
  geom_point(size = 2, color = "white", fill = "black", pch = 21) +
  facet_wrap(vars(r), labeller = label_both) +
  theme(strip.text = element_text(face = "bold", size = rel(1.3), hjust = 0))
```
:::
:::




# [Guess the correlation](http://guessthecorrelation.com)



## Correlation coefficient: just a number

. . .

It's useful in a **narrow** range of situations 

. . .

The underlying **concepts** are more broadly applicable

. . .

```{r}
tribble(~Strength, ~Direction, ~Meaning, 
        "Strong", "Positive", "As X goes up, Y goes up *a lot*", 
        "Strong", "Negative", "As X goes up, Y goes down *a lot*", 
        "Weak", "Positve", "As X goes up, Y goes up *a little*", 
        "Weak", "Negative", "As X goes up, Y goes down *a little*") %>% 
  kbl(format = "markdown") %>% 
  kable_material(c("striped", "hover"))
```


## ðŸš¨ Our turn: correlations ðŸš¨

What do these pair-wise correlations from `elections` tell us?

```{r}
elections |> 
  select(where(is.double)) |> 
  select(!contains("_2012")) |> 
  select(!contains("_2016")) |> 
  cor(use = "complete.obs") |> 
  ggcorrplot::ggcorrplot(lab = TRUE, type = "upper")
```




# Models


## Models

::: columns
::: {.column width="50%"}
- Models are everywhere in social science (and industry)


- Beneath what ads you see, movie recs, election forecasts, social science research $\rightarrow$ a **model**

- In the social sciences, we use models to describe how an **outcome** changes in response to a **treatment**

:::

::: {.column width="50%"}
```{r}
knitr::include_graphics(here::here("slides/img/models.jpeg"))
```

:::
:::


## The moving parts

::: columns
::: {.column width="50%"}

- The things we want to study in the social sciences are complex

- Why do people **vote**? We can think of lots of factors that go into this decision

- [Voted]{.red} = [Age]{.blue} + [Interest in politics]{.blue} + [Free time]{.blue} + [Social networks]{.blue} + . . . 

- Models break up (**decompose**) the variance in an [outcome]{.red} into a set of [explanatory]{.blue} variables

:::

::: {.column width="50%"}
```{r}
knitr::include_graphics(here::here("slides", "img", "model-collector.jpeg"))
```
:::
:::


## Speaking the language


```{r}
tribble(~Y, ~X,
        "Outcome variable", "Treatment variable",
        "Response variable", "Explanatory variable",
        "Dependent variable", "Independent variable",
        "What is being changed", "What is causing the change in Y") %>%
  kbl() %>% 
  row_spec(1, background = yellow) %>% 
  kable_material(c("striped", "hover"))
```

Maybe you've seen some of these terms before; here's what we will use



# ðŸš¨Your turnðŸš¨ Identify the parts

Identify the **treatment** and the **outcome** in each example below:

* A car company wants to know how changing the weight of a car will impact its fuel efficiency

* A pollster wants to predict vote choice in a county based on race, income, past voting

* Does having a friend vote make you more likely to vote? 





## Model cars

Say we want to model how changing a car's [weight]{.blue}
 (treatment) affects its [fuel efficiency]{.red} (outcome)


```{r}
mtcars |> 
  select(mpg:carb) |> 
  head(n = 5) |> 
  kbl(digits = 2, caption = "Sample from mtcars")
```


## Modeling weight and fuel efficiency


Very strong, negative correlation (**r** = -.9)

```{r}
ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, size = 1.5) + labs(x = "Weight (in 1000s of lbs)", 
                     y = "Miles/gallon (mpg)")
```


## Why a model?

. . .

Graph + correlation just gives us direction and strength of relationship

. . .

What if we wanted to know what fuel efficiency to expect if a car weighs 3 tons, or 6.2?

. . .


```{r, out.width="60%"}
ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)", 
                     y = "Miles/gallon (mpg)")
```



## Lines as models

. . .

There's many ways to model relationships between variables

. . .


A simple model is a **line** that "fits" the data "well"

. . .

The **line of best fit** describes how fuel efficiency changes as weight changes

. . .

```{r, out.width="60%"}
ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)", 
                     y = "Miles/gallon (mpg)") + 
  geom_smooth(method = "lm", color = blue)
```


## Strength of relationship

The **slope** of the line quantifies the **strength** of the relationship

. . .

Slope: -5.3 $\rightarrow$ for every ton of weight you add to a car, you lose 5 miles per gallon

. . .


```{r, out.width="60%"}
ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)", 
                     y = "Miles/gallon (mpg)") + 
  geom_smooth(method = "lm", color = blue)
```


## Guessing where there's no data

Line also gives us "best guess" for the fuel efficiency of a car with a given weight

. . .

even where we have **no data**


. . .


best guess for 4.5 ton car $\rightarrow$ 13 mpg


```{r, out.width="60%"}
ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)", 
                     y = "Miles/gallon (mpg)") + 
  geom_smooth(method = "lm", color = blue) + 
  annotate(geom = "rect", xmin = 4.1, xmax = 5.2, ymin = -Inf,ymax = Inf, 
           fill = yellow, alpha = .3)
```


## Drawing lines in R


We can add a trend-line to a graph using `geom_smooth(method = "lm")`

. . .

```{r, out.width="70%", echo= TRUE}
ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + geom_smooth(method = "lm")
```


## Where does this line come from?



::: columns
::: {.column width="50%"}
::: {.fragment}
6th grade algebra notation:

$Y = b + mx$

Y = a number

x = a number

m = the slope $\frac{rise}{run}$

b = the intercept
:::
:::

::: {.column width="50%"}
::: {.fragment}
Statistical notation:

$Y = \beta_0 + \beta_{1} x$

$Y$ = a number

$x1$ = a number

$\beta_1$ = the slope $\frac{rise}{run}$

$\beta_0$ = the intercept
:::
:::
:::

. . .


If you know $\beta_0$ (intercept) and $\beta_1$ (slope) you can draw a line


# END OF MIDTERM CONTENT {.center background-color="`r full`"}


## Some lines


```{r}
p1 = ggplot() + geom_abline(intercept = 6, slope = -2, size = 1) + 
  scale_x_continuous(limits = c(0, 10)) + 
  scale_y_continuous(limits = c(0, 10)) + 
  geom_hline(yintercept = 0, lty = 2) + 
  geom_vline(xintercept = 0, lty = 2) + 
  labs(x = "x", y = "y", title = "Y = 6 - 2x")

p2 = ggplot() + geom_abline(intercept = 1, slope = 2, size = 1) + 
  scale_x_continuous(limits = c(0, 10)) + 
  scale_y_continuous(limits = c(0, 10)) + 
  geom_hline(yintercept = 0, lty = 2) + 
  geom_vline(xintercept = 0, lty = 2) + 
  labs(x = "x", y = "y", title = "Y = 1 + 2x")

p3 = ggplot() + geom_abline(intercept = 3, slope = .05, size = 1) + 
  scale_x_continuous(limits = c(0, 10)) + 
  scale_y_continuous(limits = c(0, 10)) + 
  geom_hline(yintercept = 0, lty = 2) + 
  geom_vline(xintercept = 0, lty = 2) + 
  labs(x = "x", y = "y", title = "Y = 3 + .05x")
p1 + p2 + p3
```



## Which line to draw?

We could draw many lines through this data; which is "best"?

. . .

```{r, out.width="70%"}
ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)", 
                     y = "Miles/gallon (mpg)") + 
  geom_smooth(method = "lm", se = FALSE) + 
  geom_abline(intercept = 36, slope = -6, color = "orange", size = 1.5) + 
  geom_abline(intercept = 37, slope = -5, color = "green", size = 1.5)
```


## Drawing the line


We need to find the **intercept** ($\beta_0$) and **slope** ($\beta_1$) that will "fit" the data "well"


$mpg_{i} = \beta_0 + \beta_1 \times weight_{i}$


```{r, echo = FALSE}
ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + 
  geom_smooth(method = "lm") +  labs(x = "Weight (in 1000s of lbs)", 
                     y = "Miles/gallon (mpg)")
```



## What does it mean to "fit well"?


::: columns
::: {.column width="50%"}
- Models use a rule for defining what it means to fit the data "well"
- This rule is called a **loss function**
- Different models use different loss functions
- We'll look at the most popular modeling approach: 
- **ordinary least squares** (OLS)
:::

::: {.column width="50%"}
```{r, out.height="40%"}
knitr::include_graphics(here::here("slides/img/bad-fit-jordan.jpeg"))
```
:::
:::



## The loss function for OLS


. . .


The loss function for OLS (roughly!) says: 


. . .


a line fits the data *well* if the sum of all the squared [distances]{.blue} between the [line]{.red} and [each data point]{.yellow} is **as small as possible**


. . .


$Y_i$ is the [data point]{.yellow} $\rightarrow$ value of observation $i$ $\rightarrow$ the mpg of car $i$


. . .


$\widehat{Y_i}$ is the [line]{.red} $\rightarrow$ the mpg our line predicts for a car with that weight $\rightarrow$ 

. . .


$\beta_0 + \beta_1 \times weight$


. . .


[Distance]{.blue} between each point and line = $(Y_i - \widehat{Y_i})$ 



## Distance between each point and the line


$Y_i$ = actual MPG for the [Toyota Corolla]{.yellow}


```{r}
mpg_model = lm(mpg ~ wt, data = mtcars)

mtcars$predicted = predict(mpg_model)   # Save the predicted values
mtcars$residuals = residuals(mpg_model) # Save the residual values


ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)",
                     y = "Miles/gallon (mpg)") + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  # geom_segment(data = slice(mtcars, 20), aes(xend = wt, yend = predicted), 
  #              color = blue, size = 2) + 
  # geom_point(data = slice(mtcars, 20), aes(y = predicted), size = 4, 
  #            color = red) + 
  geom_point(data = slice(mtcars, 20), aes(y = mpg), size = 4, 
             color = yellow)
```



## Distance between each point and the line


$\widehat{Y_i}$ = our model's [predicted MPG]{.red} ($\beta_0$ + $\beta_1 \times weight$) for the Corolla


```{r}
mpg_model = lm(mpg ~ wt, data = mtcars)

mtcars$predicted = predict(mpg_model)   # Save the predicted values
mtcars$residuals = residuals(mpg_model) # Save the residual values


ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)",
                     y = "Miles/gallon (mpg)") + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  # geom_segment(data = slice(mtcars, 20), aes(xend = wt, yend = predicted), 
  #              color = blue, size = 2) + 
  geom_point(data = slice(mtcars, 20), aes(y = predicted), size = 4,
             color = red) +
  geom_point(data = slice(mtcars, 20), aes(y = mpg), size = 4, 
             color = yellow)
```


## Distance between each point and the line



$Y_i - \widehat{Y_i}$ = the [distance]{.blue} between the data point and the line


```{r}
mpg_model = lm(mpg ~ wt, data = mtcars)

mtcars$predicted = predict(mpg_model)   # Save the predicted values
mtcars$residuals = residuals(mpg_model) # Save the residual values


ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)",
                     y = "Miles/gallon (mpg)") + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  geom_segment(data = slice(mtcars, 20), aes(xend = wt, yend = predicted),
               color = blue, size = 2) +
  geom_point(data = slice(mtcars, 20), aes(y = predicted), size = 4,
             color = red) +
  geom_point(data = slice(mtcars, 20), aes(y = mpg), size = 4, 
             color = yellow)
```


## Back to the loss function


$Y_i - \widehat{Y_i}$ is the distance between the point and the line

. . .


OLS uses the **squared** distances: $(Y_i - \widehat{Y_i})^2$


. . .


The **sum** of the squared distances: $\sum_i^n{(Y_i - \widehat{Y_i})^2}$


. . . 


Remember that $\widehat{Y_i} = \widehat{\beta_0} + \widehat{\beta_{1}} \times weight$


. . .

So can rewrite: $\sum_i^n{(Y_i - \widehat{Y_i})^2} = \sum_i^n{(Y_i - \widehat{\beta_0} + \widehat{\beta_{1}} \times weight)^2}$


. . .


So we need to pick $\beta_0$ and $\beta_1$ to make this as small as possible (**minimize**):


$\sum_i^n{(Y_i - \widehat{\beta_0} + \widehat{\beta_{1}} \times weight)^2}$








## Estimating the parameters


. . .


How do we find the specific $\beta_0$ and $\beta_1$ that minimize the loss function?


. . .

Zooming out (not just OLS): in modeling there are two **kinds** of approaches

. . .

::: columns
::: {.column width="50%"}
::: {.fragment}
**Statistical theory**: make some assumptions and use math to find $\beta_0$ and $\beta_1$ 
:::
:::

::: {.column width="50%"}
::: {.fragment}
**Computational**: make some assumptions and use brute force smart guessing to find $\beta_0$ and $\beta_1$  
:::
:::
:::



. . . 


Sometimes (1) and (2) will produce the same answer; other times only (1) or (2) is possible



## Computational approach


Imagine an algorithm that:


1. Draws a line through the data by picking values of $\beta_0$ and $\beta_1$

2. Sees how far each point is from the line

3. Sum up all the **squared** distances for that line (remember OLS calls for squaring!)

4. Rinse, repeat with a new line

. . .


The pair of $\beta_0$ and $\beta_1$ that produce the line with the *smallest* sum of squared distances is our OLS estimator



## Strawman comparison

Which line fits the data better? (Duh) 


```{r}
p1 = ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)", 
       title = "Line candidate #1",
                     y = "Miles/gallon (mpg)") + 
  geom_smooth(method = "lm", se = FALSE)

p2 = ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)", 
       title = "Line candidate #2",
                     y = "Miles/gallon (mpg)") + 
  geom_hline(yintercept = mean(mtcars$mpg), color = "blue")

p1 + p2
```


## First candidate: fit the line



Rough guess: what is $\beta_0$ and $\beta_1$ here?



```{r}
p1
```



## First candidate: the residuals

How far is each point from the line?  This is $(Y_i - \widehat{Y_i})$


```{r}
# plot with residuals
mpg_model = lm(mpg ~ wt, data = mtcars)

mtcars$predicted = predict(mpg_model)   # Save the predicted values
mtcars$residuals = residuals(mpg_model) # Save the residual values


ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)",
                     y = "Miles/gallon (mpg)") + 
  geom_smooth(method = "lm", se = FALSE) + 
  geom_segment(aes(xend = wt, yend = predicted), 
               color = "orange") +  # alpha to fade lines
  geom_point(aes(y = predicted), shape = 1)
```


Also known as the **residual** $\approx$ how *wrong* the line is about each point


## How good is line 1?


Remember, our loss function says that a good line has small $(Y_i - \widehat{Y_i})^2$

. . .



```{r}
mtcars %>% 
  select(`mpg` = mpg, "\\(\\ \\widehat{mpg}\\)" = predicted, residual = residuals) %>% 
  mutate(squared_residual = residual^2) |> 
  head(n = 5) |> 
  kbl(digits = 2)
```

. . . 

We sum up the squared residuals to measure how good (or bad) the line is **overall** = $\sum_i^n{(Y_i - \widehat{Y_i})^2} \approx 278.3$  



# Second candidate: fit the line


Rough guess: what is $\beta_0$ and $\beta_1$ here?


```{r}
ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)", 
                     y = "Miles/gallon (mpg)") + 
  geom_hline(yintercept = mean(mtcars$mpg), color = "blue")
```


## Second candidate: the residuals


```{r}
ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point(color = blue, alpha = .8) + 
  labs(x = "Weight (in 1000s of lbs)", 
                     y = "Miles/gallon (mpg)") + 
  geom_hline(yintercept = mean(mtcars$mpg), color = "blue") + 
  geom_segment(aes(xend = wt, yend = mean(mtcars$mpg)), 
               color = "orange") +  # alpha to fade lines
  geom_point(aes(y = mean(mtcars$mpg)), shape = 1)
```



## How good is line 2?



```{r}
mtcars %>% 
  mutate(bad_pred = 20.09) %>% 
  mutate(resid_bad = mpg - bad_pred) %>% 
  select(`mpg` = mpg, "\\(\\ \\widehat{mpg}\\)" = bad_pred, residual = resid_bad) %>% 
  mutate(squared_residual = residual^2) |> 
  sample_n(5) |> 
  kbl(digits = 2)
```


. . .

$\sum_i^n{(Y_i - \widehat{Y_i})^2} \approx 1,126$  


So line 2 is a **much worse** fit for the data than line 1




## The best line

. . .

This is what `geom_smooth()` is doing under-the-hood (sorry)

. . .

It's using math to figure out which line fits the data best

. . .

**Conceptual** best = the line that is closest to all the points

. . .

**Math** best = the smallest sum of squared residuals


## So who are the winning Betas?



::: columns
::: {.column width="50%"}
$Y = \beta_0 + \beta1x_1$


$mpg = \beta_0 + \beta_1 \times weight$


$mpg = 37 + -5 \times weight$
:::

::: {.column width="50%"}
```{r}
p1
```

:::
:::


## What does this model get us?


$$mpg = 37 + -5weight$$

. . .


A lot

. . .

We can describe the overall rate of change with the **slope**

. . .


for every ton of weight we add to a car we **lose** 5 miles per gallon


. . .


We can also plug in whatever weight we want and get an estimate


. . .


What's the fuel efficiency of a car that weighs exactly 2 tons?

. . .


$mpg = 37 + -5 \times weight$
  

$mpg = 37 + -5 \times 2 = 27$



# Modeling in a nutshell


::: columns
::: {.column width="50%"}
- You want to see how some [outcome]{.red} responds to some [treatment]{.blue}

- Fit a line (or something else) to the variables

- Extract the underlying **model**

- Use the model to make **inferences**

:::

::: {.column width="50%"}
```{r, out.height="80%"}
knitr::include_graphics(here::here("slides/img/model2.jpeg"))
```
:::
:::
