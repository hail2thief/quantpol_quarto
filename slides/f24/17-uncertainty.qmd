---
title: "Uncertainty II"
subtitle: "POL51"
format: 
  clean-revealjs:
    incremental: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Juan Tellez
    email: jftellez@ucdavis.edu
    affiliations: UC Davis
date: last-modified
title-slide-attributes:
  data-background-image: ../img/dubois-spiral-2.png
  data-background-size: contain
  data-background-position: right
---


```{r setup, include=FALSE}
# set knit options
knitr::opts_chunk$set(
  fig.width=9, 
  fig.height=5, 
  fig.retina=3,
  fig.align="center",
  tidy = FALSE,
  tidy.opts = list(width.cutoff = 80),
  out.width = "100%",
  cache = FALSE,
  echo = FALSE,
  message = FALSE, 
  warning = FALSE
)


# source
source(here::here("R/funcs.R"))

# libraries
library(juanr)
library(moderndive)
library(paletteer)
library(huxtable)
library(wakefield)
library(socviz)


set.seed(1990)

```



# Plan for today {.center background-color="`r full`"}

. . .

. . .


Quantifying uncertainty


. . .



The confidence interval


. . .


Are we sure it's not zero?





# Where are we at? {.center background-color="`r full`"}
---


## The problem

We know our analysis is based on samples, and different samples give different answers: 


```{r}
gss_sm %>% 
  rep_sample_n(size = 10, reps = 8) %>% 
  group_by(replicate) %>% 
  summarise(childs = mean(childs, na.rm = TRUE)) %>% 
  select(`Sample` = replicate, `Avg. num of kids in sample` = childs) %>% 
  knitr::kable(digits = 2, caption = "Average number of kids in samples of size 10")
```



## The way out


::: columns
::: {.column width="50%"}
- Turns out that if our samples are **representative** of the population, then estimates from **large** samples will tend to be pretty damn close

- So if sample is good ✅ and "big" ✅ then most of the time we'll be OK ✅
:::

::: {.column width="50%"}
```{r}
kids = crossing(n = c(10, 20, 50, 100, 200, 500)) %>% 
  mutate(samples = map(.x = n, ~rep_sample_n(tbl = gss_sm, size = .x, reps = 1000))) %>% 
  unnest(samples) %>% 
  group_by(n, replicate) %>% 
  summarise(childs = mean(childs, na.rm = TRUE))

ggplot(kids, aes(x = childs)) + 
  geom_density(fill = red, alpha = .9, color = "white") + 
  facet_wrap(vars(n)) + 
  theme_nice() + 
  geom_vline(xintercept = mean(gss_sm$childs, na.rm = TRUE), 
             lty = 1, size = 1, color = blue) + 
  labs(x = "Distribution of sample average # of kids")
```
:::
:::









# But this is weird 

. . .

We’ve shown that if we take many (large) random samples, most of the averages of those samples will be close to **population parameter**

. . .

But in real life we only ever have one sample (e.g., one poll)

. . .

How do we get a sense for uncertainty from **our one sample**?




## Two approaches


::: columns
::: {.column width="50%"}
❌ ~~Statistical theory~~

* Make assumptions about distribution of samples from population
* Design test based on those assumptions (t-test, z-test, etc.)
:::

::: {.column width="50%"}
✅ Simulation

* Simulate different samples that **look like ours**
* Use distribution of simulated samples to quantify uncertainty
:::
:::

. . .

In some cases, both get you to the same answer, in others, only one works




## Simulation


We want a sense for how *uncertain* we should feel on estimates drawn from our sample

. . .


Our sample is `gss_sm`, and we have `r scales::comma(nrow(gss_sm))` observations



```{r}
gss_sm |> 
  select(1:8) |> 
  sample_n(5) |> 
  kbl(digits = 0, caption = "gss_sm")
```


. . .


How confident are we in the estimate we get from this sample, given its **size**?








## How to simulate


If we take lots of samples of size 20 $\rightarrow$ uncertainty in a sample of 20

```{r, echo = TRUE, eval = FALSE}
gss_sm %>%
  rep_sample_n(size = 20, reps = 1000)
```


```{r, echo = FALSE, eval = TRUE}
kids |> 
  filter(n == 20) |> 
  ggplot(aes(x = childs)) + 
  geom_density(fill = red, alpha = .9, color = "white") + 
  theme_nice() + 
  geom_vline(xintercept = mean(gss_sm$childs, na.rm = TRUE), 
             lty = 1, size = 2, color = blue) + 
  labs(x = "Distribution of sample average # of kids") + 
  coord_cartesian(xlim = c(0.5, 3.5))
```



## How to simulate


If we take lots of samples of size 100 $\rightarrow$ uncertainty in a sample of 100

```{r, echo = TRUE, eval = FALSE}
gss_sm %>%
  rep_sample_n(size = 100, reps = 1000)
```


```{r, echo = FALSE, eval = TRUE}
kids |> 
  filter(n == 100) |> 
  ggplot(aes(x = childs)) + 
  geom_density(fill = red, alpha = .9, color = "white") + 
  theme_nice() + 
  geom_vline(xintercept = mean(gss_sm$childs, na.rm = TRUE), 
             lty = 1, size = 2, color = blue) + 
  labs(x = "Distribution of sample average # of kids") + 
  coord_cartesian(xlim = c(0.5, 3.5))
```



## How to simulate


So to see how uncertain we should feel about `gss_sm`, we should take many samples that are **the same size** as `gss_sm`

. . .


```{r, echo = TRUE, eval = FALSE}
gss_sm %>% 
  rep_sample_n(size = 2867, reps = 1000)
```

OR 

. . .

```{r, echo = TRUE, eval = FALSE}
gss_sm %>% 
  rep_sample_n(size = nrow(gss_sm), reps = 1000)
```




::: {.callout-note}
`nrow(DATA)` tells you how many observations in data object
:::



## Problem


If we have a dataset of 2,867 observations and ask R to randomly pick 2,867 observations, we'll just get a bunch of copies of the original dataset


```{r, echo = TRUE, eval = FALSE}
gss_sm %>% 
  rep_sample_n(size = nrow(gss_sm), reps = 1000)
```


. . .


Solution: sample **with replacement** $\rightarrow$ once we draw an observation it goes back into the dataset, and can be sampled again


. . .


```{r, echo = TRUE, eval = FALSE}
gss_sm %>% 
  rep_sample_n(size = nrow(gss_sm), reps = 1000, replace = TRUE)
```


::: {.callout-note}
`replace = TRUE` allows an observation to be re-sampled
:::



## Sampling with replacement


```{r}
knitr::include_graphics(here::here("slides/img/with-replacement.png"))
```




## Sampling with and without replacement

If we were sampling **4** of these delicious fruits: 

```{r, echo = FALSE}
food = tibble(fruits = c("Mango", "Pineapple", "Banana", 
                         "Blackberry"))
food |> kbl(caption = "Dataset on fruits")
```

## Sampling with and without replacement



Without replacement:

```{r, echo = TRUE}
rep_sample_n(food, size = 4, reps = 1)
```

. . .

With replacement:

```{r, echo = TRUE}
rep_sample_n(food, size = 4, reps = 1, replace = TRUE)
```







## Bootstrapping


::: columns
::: {.column width="50%"}
* Generating many, **same-sized** samples with replacement is called **bootstrapping**
* Replacement lets us generate samples that randomly differ from ours
* Use the distribution of bootstrapped samples to quantify uncertainty
:::

::: {.column width="50%"}
```{r, fig.cap="Only one sample? Pull yourself up by your bootstraps!"}
knitr::include_graphics(here::here("slides/img/cowboy-spongebob.png"))
```
:::
:::



## Back to the kids


How uncertain should we be of our estimate of the avg. number of kids in the US, Given that it's based on our one sample, `gss_sm`? We can bootstrap:


```{r, echo = TRUE}
boot_kids = gss_sm %>% 
  rep_sample_n(size = nrow(gss_sm), reps = 1000, replace = TRUE) %>% 
  summarise(avg_kids = mean(childs, na.rm = TRUE))
boot_kids
```



## The distribution of bootstrapped estimates

Our [estimate]{.blue} and how much [simulated estimates]{.red} might vary across bootstrapped samples that **look like ours**

```{r}
p1 = ggplot(boot_kids, aes(x = avg_kids)) + 
  geom_histogram(fill = red, color = "white") + 
  theme_nice() + 
  labs(x = "Average number of kids across bootstraps") + 
  geom_vline(xintercept = mean(gss_sm$childs, na.rm = TRUE), size = 2.5, 
             color = blue)
p1
```


The red is the distribution of bootstrapped sample estimates $\rightarrow$ the **sampling distribution**



## Your turn: Income and household assets


`wealth` is all dummy variables, tell you whether household in Honduras 🇭🇳 has a particular asset or not:

```{r}
wealth |> 
  sample_n(5) |> 
  knitr::kable(digits = 0, caption = "Sample from `wealth`")
```


## Your turn: Income and household assets


Using `wealth` from `juanr`, pick an asset from the codebook:


1. What percent of households own that asset? 

2. How uncertain should you be of your estimate? Generate 1,000 bootstraps and plot the distribution.


```{r}
#| echo: false
  countdown::countdown(minutes = 10L)
```



# Quantifying uncertainty {.center background-color="`r full`"}
---


## How to quantify uncertainty?



::: columns
::: {.column width="50%"}
The red histogram is nice, but how can we communicate uncertainty in our estimates in a pithy, more comparable way?

Three approaches:

* The standard error
* The confidence interval
* Statistical significance
:::

::: {.column width="50%"}
```{r}
p1
```
:::
:::





## The standard error


One way to quantify uncertainty would be to measure how "wide" the distribution of bootstrapped sample estimates is


. . .


As we learned so long ago, one way to measure the "spread" of a distribution (i.e., how much a variable **varies**), is with the *standard deviation*


. . .


The standard deviation of the **sampling distribution** is called the **standard error**, or the **margin of error**



## The standard error


Generate bootstraps:


```{r, echo = TRUE}
boot_kids = gss_sm %>% 
  rep_sample_n(size = nrow(gss_sm), reps = 1000, replace = TRUE) %>% 
  summarise(avg_kids = mean(childs, na.rm = TRUE))
```

Calculate the mean and standard error of the bootstraps:


```{r, echo = TRUE}
boot_kids %>% 
  summarise(mean = mean(avg_kids), standard_error = sd(avg_kids))
```



Best guess? About 1.85 kids, +/- 2 standard errors (1.85 - 2 * .03 = 1.79, 1.85 + 2 * .03 = 1.91)




## Standard error


This is what you see in the news -- that +/- polling/margin of error



```{r}
knitr::include_graphics(here::here("slides/img/margin-of-error.png"))
```


## Varying uncertainty


Standard errors get smaller as sample sizes get larger:

```{r}
crossing(size = round(seq(10, 500, length.out = 10), 0)) |> 
  mutate(data = map(.x = size, .f = ~ rnorm(n = .x, mean = 10, sd = 2))) |> 
  unnest() |> 
  group_by(size) |> 
  summarise(mean = mean(data), 
            standard_error = (sd(data) / sqrt(size))) |> 
  distinct() |> 
  select(`Sample size` = size, `Average (truth = 10)` = mean, `Standard error` = standard_error) |> 
  kbl(digits = 2)
```




# The confidence interval {.center background-color="`r full`"}



## The confidence interval


Another way to quantify uncertainty is to look where **most** estimates fall


this is the **confidence interval**: our "best guess" of what we're trying to estimate


```{r, out.width="80%"}
ggplot(boot_kids, aes(x = avg_kids)) + 
  geom_histogram(fill = red, color = "white") + 
  theme_nice() + 
  labs(x = "Average number of kids across bootstraps") +
  annotate(geom = "rect", 
           xmin = quantile(boot_kids$avg_kids, probs = c(.025)), 
           xmax = quantile(boot_kids$avg_kids, probs = c(.975)), 
           ymin = 0, ymax = Inf, 
           alpha = .5, fill = yellow)
```




## How big to make the interval?


You could report (for example) where the middle `r kableExtra::text_spec("50%", color = red, bold = TRUE)` of bootstraps fall, or (for example) where the middle `r kableExtra::text_spec("95%", color = yellow, bold = TRUE)` of bootstraps fall, but there are **tradeoffs**!


```{r}
p1 = ggplot(boot_kids, aes(x = avg_kids)) + 
  geom_histogram(color = "white") + 
  theme_nice() + 
  labs(x = "Average number of kids across bootstraps") +
  annotate(geom = "rect", 
           xmin = quantile(boot_kids$avg_kids, probs = c(.025)), 
           xmax = quantile(boot_kids$avg_kids, probs = c(.975)), 
           ymin = 0, ymax = Inf, 
           alpha = .5, fill = yellow) + 
  annotate(geom = "rect", 
           xmin = quantile(boot_kids$avg_kids, probs = c(.25)), 
           xmax = quantile(boot_kids$avg_kids, probs = c(.75)), 
           ymin = 0, ymax = Inf, 
           alpha = .5, fill = red)
p1
```




## The tradeoff

* You are [50%]{.red} "confident" that avg. number of kids could vary between `r round(quantile(boot_kids$avg_kids, probs = c(.25)), 2)` and `r round(quantile(boot_kids$avg_kids, probs = c(.75)), 2)`. Narrower range! But low confidence!

* You are [95%]{.yellow} "confident" that avg. number of kids could vary between `r round(quantile(boot_kids$avg_kids, probs = c(.025)), 2)` and `r round(quantile(boot_kids$avg_kids, probs = c(.975)), 2)`. Higher range! But higher confidence!


```{r}
p1
```




## How big to make the interval?


**Convention** is to look at the middle **95%** of the distribution


. . .


We can use the `quantile()` function to find the upper and lower bound of the middle 95%:


```{r, echo = TRUE}
boot_kids %>% 
  summarise(low = quantile(avg_kids, .025), # middle 95% means lower bound is .025
            mean = mean(avg_kids), 
            high = quantile(avg_kids, .975)) # middle 95% means upper bound is .975
```

. . .


The 95% confidence confidence interval for the average number of kids in the US is: (1.80, 1.91)




::: {.notes}
draw on board with middle 50% of distribution
:::




## Mirrors of one another


The standard error and confidence interval are actually telling you the same thing

. . .


A 95% confidence interval is roughly equal to the Estimate +/- 1.96 $\times$ standard error


::: columns
::: {.column width="50%"}
```{r, echo = TRUE}
boot_kids %>% 
  summarise(low = quantile(avg_kids, .025),
            mean = mean(avg_kids), 
            high = quantile(avg_kids, .975))
```
:::

::: {.column width="50%"}
```{r, echo = TRUE}
boot_kids %>% 
  summarise(mean = mean(avg_kids), standard_error = sd(avg_kids)) %>% 
  mutate(low = mean - 1.96 * standard_error, 
         high = mean + 1.96 * standard_error) %>% 
  select(low, mean, high)
```
:::
:::



## 🚨 Your turn: Wealth data 🚨

Look back at the `wealth` data 


1. Grab your bootstrapped samples for the asset of your choosing.

2. Calculate the standard error and the 95% confidence interval of your best guess. Convince yourself the two can be made equivalent. 



```{r}
countdown::countdown(minutes = 10L, font_size = "2em")
```





